{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 43043: expected 2 fields, saw 3\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "Found 88548 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data1 = pd.read_csv('labeledTrainData.tsv', sep='\\t', encoding = 'ISO-8859-1')\n",
    "data1['review'] = data1['review'].str.replace('<br /><br />', ' ')\n",
    "\n",
    "data2 = pd.read_csv('unlabeledTrainData.tsv', sep='\\t', encoding = 'ISO-8859-1', error_bad_lines=False)\n",
    "data2['review'] = data2['review'].str.replace('<br /><br />', ' ')\n",
    "\n",
    "data3 = pd.read_csv('testData.tsv', sep='\\t', encoding = 'ISO-8859-1')\n",
    "data3['review'] = data3['review'].str.replace('<br /><br />', ' ')\n",
    "\n",
    "data4 = pd.read_csv('test.csv')\n",
    "data4['review'] = data4['review'].str.replace('<br /><br />', ' ')\n",
    "texts = list(data1['review']) + list(data4['review'])\n",
    "print(len(texts))\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "x_train = data1['review']\n",
    "y_train = data1['sentiment']\n",
    "x_test = data4['review']\n",
    "y_test = data4['sentiment']\n",
    "\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "20000 train sequences\n",
      "5000 test sequences\n",
      "Average train sequence length: 227\n",
      "Average test sequence length: 225\n",
      "Adding 2-gram features\n",
      "Average train sequence length: 453\n",
      "Average test sequence length: 413\n",
      "1062002\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "ngram_range = 2\n",
    "# max_features = 20000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 6\n",
    "\n",
    "print('Loading data...')\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(\n",
    "    np.mean(list(map(len, x_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(\n",
    "    np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_test)), dtype=int)))\n",
    "print(max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "20000 train sequences\n",
      "5000 test sequences\n",
      "Average train sequence length: 227\n",
      "Average test sequence length: 225\n",
      "Adding 3-gram features\n",
      "Average train sequence length: 678\n",
      "Average test sequence length: 514\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (20000, 500)\n",
      "x_test shape: (5000, 500)\n",
      "Build model...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/6\n",
      "20000/20000 [==============================] - 1885s 94ms/step - loss: 0.6383 - acc: 0.7759 - val_loss: 0.5101 - val_acc: 0.8362\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.83620, saving model to ./fasttext2.model\n",
      "Epoch 2/6\n",
      "20000/20000 [==============================] - 1768s 88ms/step - loss: 0.3214 - acc: 0.9542 - val_loss: 0.3372 - val_acc: 0.8774\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.83620 to 0.87740, saving model to ./fasttext2.model\n",
      "Epoch 3/6\n",
      "20000/20000 [==============================] - 1675s 84ms/step - loss: 0.1113 - acc: 0.9905 - val_loss: 0.2865 - val_acc: 0.8946\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.87740 to 0.89460, saving model to ./fasttext2.model\n",
      "Epoch 4/6\n",
      "20000/20000 [==============================] - 1654s 83ms/step - loss: 0.0485 - acc: 0.9978 - val_loss: 0.2667 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.89460 to 0.89740, saving model to ./fasttext2.model\n",
      "Epoch 5/6\n",
      "20000/20000 [==============================] - 1757s 88ms/step - loss: 0.0252 - acc: 0.9993 - val_loss: 0.2569 - val_acc: 0.8980\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.89740 to 0.89800, saving model to ./fasttext2.model\n",
      "Epoch 6/6\n",
      "20000/20000 [==============================] - 1677s 84ms/step - loss: 0.0146 - acc: 0.9997 - val_loss: 0.2508 - val_acc: 0.8998\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.89800 to 0.89980, saving model to ./fasttext2.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feb09218390>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('./fasttext2.model', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "test_id = list(data3['id'])\n",
    "print(len(test_id))\n",
    "out = open('submission.csv', 'w')\n",
    "out.write('id,sentiment\\n')\n",
    "for i in range(len(result)):\n",
    "    out.write(test_id[i] + ',' + str(int(np.rint(result[i][0]))) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
